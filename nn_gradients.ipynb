{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks: Backpropagation of Gradients on Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Backpropagation of Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Neural networks are trained by minimization of a loss function $L(\\vec{y})$ of outputs $\\vec{y}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In each minimization step, trainable variables must be varied in the opposite direction of the loss gradient $\\nabla L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"fig/example_graph.png\" alt=\"drawing\" width=\"700\"/>\n",
    "How does one vary, for example, trainable parameter $W_i$ in the direction of $-\\nabla L$?\n",
    "\n",
    "$W_{i} \\rightarrow W_{i} - \\frac{\\partial L}{\\partial W_{i}}\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation of Gradients\n",
    "Backpropagation on a graph is essentially the chain rule:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_{i}} = \\sum_{j}\\frac{\\partial L}{\\partial z_{j}}\\sum_{k}\\frac{\\partial z_{j}}{\\partial y_{k}}\\frac{\\partial y_{k}}{\\partial W_{i}} = \\sum_{j,k}\\frac{\\partial L}{\\partial z_{j}}\\frac{\\partial z_{j}}{\\partial y_{k}}\\frac{\\partial y_{k}}{\\partial W_{i}} = \\sum_{j,k}\\frac{\\partial y_{k}}{\\partial W_{i}}\\frac{\\partial z_{j}}{\\partial y_{k}}\\frac{\\partial L}{\\partial z_{j}}$\n",
    "\n",
    "$ = \\sum_{j,k}\\frac{\\partial y_{k}}{\\partial W_{i}}\\Bigl[\\frac{\\partial z_{j}}{\\partial y_{k}}\\Bigl(\\frac{\\partial L}{\\partial z_{j}}\\Bigr)\\Bigr] = \\sum_{k}\\frac{\\partial y_{k}}{\\partial W_{i}}\\sum_{j}\\Bigl[\\frac{\\partial z_{j}}{\\partial y_{k}}dz_{j}\\Bigr]$, where $dz_{j} = \\frac{\\partial L}{\\partial z_{j}}$\n",
    "\n",
    "$ = \\sum_{k}\\frac{\\partial y_{k}}{\\partial W_{i}} dy_{k}$, where $dy_{k} = \\sum_{j}\\Bigl[\\frac{\\partial z_{j}}{\\partial y_{k}}dz_{j}\\Bigr]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Both Tensorflow and PyTorch perform the chain rule computation by *backpropagation* of the gradients through the neural network.\n",
    "<img src=\"fig/example_graph_gradients.png\" alt=\"drawing\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation in Tensorflow\n",
    "- Tensorflow automatically calculates gradients of a graph\n",
    "- \"Automatically\", meaning gradients are propagated to any point in the graph *given that the gradients of all operations in the graph have been clearly defined*\n",
    "- For each operation, one must define how to send gradients back through the inputs, given the output gradients.  How is this done?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To calculate the derivative of the loss: \n",
    "- with respect to trainable variable $W_i$ (or input $x_i$)\n",
    "- given output gradients $dy_{k}$\n",
    "\n",
    "Compute the sum:\n",
    "\n",
    "$dW_{i} = \\frac{\\partial L}{\\partial W_{i}} = \\sum_{k}\\frac{\\partial y_{k}}{\\partial W_{i}}dy_{k}$\n",
    "\n",
    "Or for input variable $x_i$, the gradient to send backward one step is:\n",
    "\n",
    "$dx_{i} = \\frac{\\partial L}{\\partial x_{i}} = \\sum_{k}\\frac{\\partial y_{k}}{\\partial x_{i}}dy_{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercises:\n",
    "- Here we compute gradients for several operations used frequently in neural networks. \n",
    "- We'll start with one in which there is exactly one output for each input, and each output depends only on its corresponding input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. The sigmoid function: $\\sigma(x_i) = 1/(1+e^{-x_i})$\n",
    "\n",
    "![example_sigmoid.png](fig/example_sigmoid.png)\n",
    "\n",
    "Compute $dx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define inputs and output gradients.\n",
    "x = tf.constant([3.0, 4.0, 5.0])\n",
    "dz = tf.constant([1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the gradient.\n",
    "def grad_sigmoid(x, dz):\n",
    "    # (Add implementation here)\n",
    "dx = grad_sigmoid(x,dz)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Compute the gradient with Tensorflow.\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    z = tf.sigmoid(x)\n",
    "dx_tf = g.gradient(z, x, output_gradients=dz)\n",
    "print(dx_tf)\n",
    "print(dx == dx_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$z_i = \\sigma(x_i) = 1/(1+e^{-x_i})$\n",
    "\n",
    "$\\frac{\\partial[\\vec{\\sigma}(\\vec{x})]_i}{\\partial x_j} = \\frac{-1}{(1 + e^{-x_i})^2}\\cdot e^{-x_i}\\cdot(-1)\\cdot\\frac{\\partial x_i}{\\partial x_j}$\n",
    "\n",
    "$= \\frac{1}{(1 + e^{-z})}\\cdot\\frac{e^{-x_i}}{(1+e^{-x_i})}\\cdot\\delta_{ij}$\n",
    "\n",
    "$= \\sigma(x_{i})\\cdot\\frac{1 + e^{-x_i} - 1}{(1+e^{-x_i})}\\cdot\\delta_{ij}$\n",
    "\n",
    "$= \\sigma(x_{i})(1-\\sigma(x_{i}))\\delta_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$dx_j = \\sum_i dz_i \\frac{\\partial\\sigma(x)_i}{\\partial x_j}$\n",
    "\n",
    "$ = \\sum_i dz_i \\sigma(x_{i})(1-\\sigma(x_{i}))\\delta_{ij}$\n",
    "\n",
    "$ = dz_j \\sigma(x_{j})(1-\\sigma(x_{j}))$\n",
    "\n",
    "or using the vector dot product,\n",
    "\n",
    "$d\\vec{x} = d\\vec{z} \\cdot d\\vec{\\sigma}(x)$\n",
    "\n",
    "where \n",
    "\n",
    "$[d\\vec{\\sigma}(x)]_i = \\sigma(x_{i})(1-\\sigma(x_{i}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Possible implementation of sigmoid gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def grad_sigmoid(x, dz):\n",
    "    return dz*tf.sigmoid(x)*(1-tf.sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What if each output now depends on more than one input?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. The softmax function $\\sigma_s(x_i) = e^{x_i}/\\sum_{j}{e^{x_j}}$\n",
    "![example_softmax.png](fig/example_softmax.png)\n",
    "\n",
    "Compute $dx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define inputs\n",
    "x = tf.constant([3.0, 4.0, 5.0])\n",
    "dz = tf.constant([1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the gradient.\n",
    "def grad_softmax(x, dz):\n",
    "    # (Add implementation here)\n",
    "dx = grad_softmax(x, dz)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Compute the gradient with Tensorflow.\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    z = tf.nn.softmax(x)\n",
    "dx_tf = g.gradient(z, x, output_gradients=dz)\n",
    "print(dx_tf)\n",
    "print(dx == dx_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$z_i = [\\sigma_s(x)]_i = e^{x_i}/\\sum_{j}{e^{x_j}}$\n",
    "\n",
    "$\\frac{\\partial[\\vec{\\sigma_s}(\\vec{x})]_i}{\\partial x_j} = \\frac{e^{x_i}\\delta_{ij}}{\\sum_k e^{x_k}} - \\frac{e^{x_i}\\cdot\\sum_k e^{x_k}\\delta_{jk}}{\\bigl(\\sum_{k}{e^{x_k}}\\bigr)^2}$\n",
    "\n",
    "$= \\frac{e^{x_i}\\delta_{ij}}{\\sum_k e^{x_k}} - \\frac{e^{x_i}e^{x_j}}{\\bigl(\\sum_{k}{e^{x_k}\\bigr)^2}}$\n",
    "\n",
    "$= [\\sigma_s(x)]_i\\delta_{ij} - [\\sigma_s(x)]_i[\\sigma_s(x)]_j$\n",
    "\n",
    "$= [\\sigma_s(x)]_i(\\delta_{ij} - [\\sigma_s(x)]_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$dx_j = \\sum_i dz_i \\frac{\\partial\\sigma(x)_i}{\\partial x_j}$\n",
    "\n",
    "$ = \\sum_i dz_i [\\sigma_s(x)]_i(\\delta_{ij} - [\\sigma_s(x)]_j)$\n",
    "\n",
    "$ = dz_j [\\sigma_s(x)]_j - dz\\cdot\\sigma_s[\\sigma_s(x)]_j$\n",
    "\n",
    "$ = [\\sigma_s(x)]_j(dz_j - d\\vec{z}\\cdot\\vec{\\sigma_s})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Possible implementation of the softmax gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def grad_softmax(x,dz):\n",
    "    return tf.nn.softmax(x)*(dz - tf.tensordot(tf.nn.softmax(x), dz, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So far, we have considered vector-valued inputs.\n",
    "![tensor_inputs_outputs.png](fig/tensor_inputs_outputs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In general, we have tensor-valued inputs with multiple indices.\n",
    "![vector_and_tensor.png](fig/vector_and_tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Matrix multiplication: $z_{ij} = \\sum_k{x_{ik}y_{kj}}$\n",
    "![example_matrix_mult.png](fig/example_matrix_mult.png)\n",
    "\n",
    "Compute $dx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define inputs\n",
    "x = tf.constant([[3.0, 4.0], [5.0, 6.0]])\n",
    "y = tf.constant([[4.0, 5.0], [6.0, 7.0]])\n",
    "dz = tf.constant([[1.0, 2.0], [3.0, 4.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Define the gradient.\n",
    "def grad_matmul(x, y, dz):\n",
    "    # (Add implementation here)\n",
    "dx = grad_matmul(x, y, dz)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# Compute the gradient with Tensorflow.\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(x)\n",
    "    z = tf.matmul(x, y)\n",
    "dx_tf = g.gradient(z, x, output_gradients=dz)\n",
    "print(dx_tf)\n",
    "print(dx == dx_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$z_{ij} = \\sum_k{x_{ik}y_{kj}}$\n",
    "\n",
    "$\\frac{\\partial z_{ij}}{\\partial x_{lm}} = \\sum_k{\\delta_{il}\\delta_{km}}y_{kj}$\n",
    "\n",
    "$= \\delta_{il}y_{mj}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$dx_{lm} = \\sum_{ij} dz_{ij} \\frac{\\partial z_{ij}}{\\partial x_{lm}}$\n",
    "\n",
    "$ = \\sum_{ij} dz_{ij} \\delta_{il}y_{mj}$\n",
    "\n",
    "$ = \\sum_{j}dz_{lj}y_{mj}$\n",
    "\n",
    "$ = \\sum_{j}dz_{lj}y^{T}_{jm}$\n",
    "\n",
    "Note that this is the matrix multiplication:\n",
    "\n",
    "$dx = dz \\times y^{T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Possible implementation of the matrix multiplication gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def grad_matmul(x, y, dz):\n",
    "    return tf.matmul(dz,tf.transpose(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- An operation such as convolution may require special care because it depends on several factors (strides, padding, filter sizes, etc.)\n",
    "- We'll try a specific example, but note that the general case is more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 4. 2D Convolution $x * w$ ($k\\times l$ filter): $z_{ij} = \\sum_{kl}{w_{kl}x_{i+k,j+l}}$\n",
    "![convolution.png](fig/example_convolution.png)\n",
    "Compute $dw$. Assume for this exercise a stride of 1 in each dimension, and ignore padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Use this method to perform the convolution.\n",
    "def conv2d(x, w):\n",
    "    return tf.nn.conv2d(tf.reshape(x,[1,x.shape[0],x.shape[1],1]),\n",
    "                        tf.reshape(w,[w.shape[0],w.shape[1],1,1]), \n",
    "                        strides=[1,1],\n",
    "                        padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# 2. Define inputs\n",
    "x = tf.constant([[3.0, 4.0, 5.0, 6.0], \n",
    "                 [4.0, 5.0, 6.0, 7.0],\n",
    "                 [5.0, 6.0, 7.0, 8.0],\n",
    "                 [6.0, 7.0, 8.0, 9.0]])\n",
    "w = tf.constant([[1.0, 2.0], \n",
    "                 [3.0, 4.0]])\n",
    "dz = tf.constant([[1.0, 1.0, 1.0], \n",
    "                  [2.0, 2.0, 2.0],\n",
    "                  [3.0, 3.0, 3.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Define the gradient.\n",
    "def grad_conv2d(x, w, dz):\n",
    "    # (Add implementation here)\n",
    "dx = grad_conv2d(x, w, dz)\n",
    "dx = tf.reshape(dx, [w.shape[0],w.shape[1]]) # Reshape to remove channel and batch number dimensions.\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "# 4. Compute the gradient with Tensorflow.\n",
    "with tf.GradientTape() as g:\n",
    "    g.watch(w)\n",
    "    z = conv2d(x,w)\n",
    "dx_tf = g.gradient(z, w, \n",
    "         output_gradients=tf.reshape(dz,[1,dz.shape[0],dz.shape[1],1]))\n",
    "print(dx_tf)\n",
    "print(dx == dx_tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$z_{ij} = \\sum_{kl}{w_{kl}x_{i+k,j+l}}$\n",
    "\n",
    "$\\frac{\\partial z_{ij}}{\\partial w_{mn}} = \\sum_{kl}{\\delta_{km}\\delta_{ln}}x_{i+k,j+l}$\n",
    "\n",
    "$= x_{i+m,j+n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$dw_{mn} = \\sum_{ij} dz_{ij} \\frac{\\partial z_{ij}}{\\partial w_{mn}}$\n",
    "\n",
    "$ = \\sum_{ij} dz_{ij}x_{i+m,j+n}$\n",
    "\n",
    "$ = \\sum_{ij} dz_{ij}x_{m+i,n+j}$\n",
    "\n",
    "Note that this is the convolution:\n",
    "\n",
    "$dw = x * dz$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Possible implementation of the 2D convolution gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def grad_conv2d(x, w, dz):\n",
    "    return conv2d(x, dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
